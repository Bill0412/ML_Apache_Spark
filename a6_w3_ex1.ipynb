{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to exercise one of week three of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise we’ll use the HMP dataset again and perform some basic operations using Apache SparkML Pipeline components.\n",
    "\n",
    "Let’s create our DataFrame again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-17 12:40:39--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
      "Resolving github.com (github.com)... 52.74.223.119\n",
      "Connecting to github.com (github.com)|52.74.223.119|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n",
      "--2020-02-17 12:40:40--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.128.133, 151.101.64.133, 151.101.0.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.128.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 932997 (911K) [application/octet-stream]\n",
      "Saving to: ‘hmp.parquet’\n",
      "\n",
      "hmp.parquet         100%[===================>] 911.13K   573KB/s    in 1.6s    \n",
      "\n",
      "2020-02-17 12:40:42 (573 KB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below is the feature engineering pipeline from the lecture. Please add a feature column called “features_minmax” using the MinMaxScaler.\n",
    "\n",
    "More information can be found here:\n",
    "http://spark.apache.org/docs/latest/ml-features.html#minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+--------------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|     features_minmax|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+--------------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|[0.26684636118598...|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|[0.26684636118598...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|[0.25950196592398...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|[0.25950196592398...|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|[0.25233644859813...|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|[0.26435246995994...|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|[0.24489795918367...|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|[0.26190476190476...|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|[0.26684636118598...|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|[0.26190476190476...|\n",
      "| 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|[0.25714285714285...|\n",
      "| 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|[0.24725274725274...|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|[0.26213592233009...|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|[0.26213592233009...|\n",
      "| 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|[0.24258760107816...|\n",
      "| 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|[0.22913719943422...|\n",
      "| 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|[0.24186704384724...|\n",
      "| 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|[0.19972260748959...|\n",
      "| 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|[0.22040816326530...|\n",
      "| 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|[0.22913719943422...|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "\n",
    "minmaxscaler = MinMaxScaler(inputCol=\"features_norm\", outputCol=\"features_minmax\")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,minmaxscaler])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between a transformer and an estimator is state. A transformer is stateless whereas an estimator keeps state. Therefore “VectorAsselmbler” is a transformer since it only need to read row by row. Normalizer, on the other hand need to compute statistics on the dataset before, therefore it is an estimator. An estimator has an additional “fit” function. “OneHotEncoder” has been deprecated in Spark 2.3, therefore please change the code below to use the OneHotEstimator instead of the “OneHotEncoder”.\n",
    "\n",
    "More information can be found here:\n",
    "http://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n",
      "| 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n",
      "| 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
      "| 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n",
      "| 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n",
      "| 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n",
      "| 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n",
      "| 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n",
      "| 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler, OneHotEncoderEstimator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
