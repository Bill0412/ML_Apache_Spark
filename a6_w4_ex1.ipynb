{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to exercise one of week four of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise we’ll work on classification.\n",
    "\n",
    "Let’s create our DataFrame again:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ\n",
    "\n",
    "Then, please feel free to ask:\n",
    "\n",
    "https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n",
    "\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.5 in /home/fenghe/anaconda3/lib/python3.7/site-packages (2.4.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/fenghe/anaconda3/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-15 11:16:56--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
      "--2020-08-15 11:16:58--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
      "--2020-08-15 11:16:58--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 932997 (911K) [application/octet-stream]\n",
      "Saving to: ‘hmp.parquet’\n",
      "\n",
      "hmp.parquet         100%[===================>] 911.13K  3.27MB/s    in 0.3s    \n",
      "\n",
      "2020-08-15 11:17:00 (3.27 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 50| 38|Accelerometer-201...|Brush_teeth|\n",
      "| 19| 50| 36|Accelerometer-201...|Brush_teeth|\n",
      "| 38| 62| 23|Accelerometer-201...|Brush_teeth|\n",
      "| 40| 62| 20|Accelerometer-201...|Brush_teeth|\n",
      "| 51| 59| 32|Accelerometer-201...|Brush_teeth|\n",
      "| 40| 50| 48|Accelerometer-201...|Brush_teeth|\n",
      "| 23| 53| 51|Accelerometer-201...|Brush_teeth|\n",
      "| 48| 53| 38|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 30| 54| 26|Accelerometer-201...|Brush_teeth|\n",
      "| 30| 44| 29|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 31| 44| 43|Accelerometer-201...|Brush_teeth|\n",
      "| 32| 57| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 30| 49| 31|Accelerometer-201...|Brush_teeth|\n",
      "| 28| 37| 37|Accelerometer-201...|Brush_teeth|\n",
      "|  4| 45| 47|Accelerometer-201...|Brush_teeth|\n",
      "| 23| 53| 39|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 48| 47|Accelerometer-201...|Brush_teeth|\n",
      "|  6| 44| 37|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.01).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is supervised learning, let’s split our data into train (80%) and test (20%) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can re-use our feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use LogisticRegression, a simple and basic linear classifier to obtain a classification performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the schema of the prediction dataframe we see that there is an additional column called prediction which contains the best guess for the class our model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_norm: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s evaluate performance by using a build-in functionality of Apache SparkML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20558671752299754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get 20% right. This is not bad for a baseline. Note that random guessing would give us only 7%. Of course we need to improve. You might have notices that we’re dealing with a time series here. And we’re not making use of that fact right now as we look at each training example only individually. But this is ok for now. More advanced courses like “Advanced Machine Learning and Signal Processing” (https://www.coursera.org/learn/advanced-machine-learning-signal-processing/) will teach you how to improve accuracy to the nearly 100% by using algorithms like Fourier transformation or wavelet transformation. But let’s skip this for now. In the following cell, please use the RandomForest classifier (you might need to play with the “numTrees” parameter) in the code cell below. You should get an accuracy of around 44%. More on RandomForest can be found here:\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, VectorIndexer, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "# featureIndexer = VectorIndexer(inputCol=\"features_norm\", outputCol=\"indexed_features_norm\", maxCategories=4)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_norm\", numTrees=10)\n",
    "\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=indexer.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, labelIndexer, featureIndexer, rf])\n",
    "\n",
    "model = pipeline.fit(df_train)\n",
    "\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "\n",
    "# predictions.select(\"predictedLabel\", \"class\", \"features\").show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_norm: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- indexed_features_norm: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4263293695310747"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list()\n",
    "r = range(2, 7)\n",
    "for nTrees in r:\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_norm\", numTrees=nTrees)\n",
    "    model = pipeline.fit(df_train)\n",
    "    predictions = model.transform(df_test)\n",
    "    res.append(MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(r), res)\n",
    "plt.xlabel(\"numTrees\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
